{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a66d93",
   "metadata": {},
   "source": [
    "# Neural Nets from scratch, in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce91e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, elementwise_grad, jacobian, numpy as autograd_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a923e5f",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd63c0",
   "metadata": {},
   "source": [
    "LeCun et. al.'s seminal [LeNet-5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) is a fantastic basic network to grok the implementation of the math behind neural nets. To begin with, we define a set of activation functions that add non-linearity to what is otherwise essentially a large linear regression solver:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f323",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515a1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relu(x):\n",
    "    return autograd_np.maximum(0, x)\n",
    "\n",
    "def _sigmoid(x):\n",
    "    # My definition of a (highly numerically unstable) sigmoid function. Redefined below with an implementation that is more stable.\n",
    "    x_clip = autograd_np.clip(x, -700, 700)\n",
    "    return 1 / (1 + autograd_np.exp(-x_clip))\n",
    "\n",
    "\n",
    "activation_function_map = {\n",
    "    'relu': _relu,\n",
    "    'sigmoid': _sigmoid,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896ac5d",
   "metadata": {},
   "source": [
    "Next, we define loss functions to measure the quality of the output layer. Since this is a classification problem, we need class membership probabilities; the sigmoid function is used as the activation of the final layer for this. The loss function, therefore, must compare this class membership probability vector against the label (ground truth) for that training observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5f157",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9c5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crossentropy(y, y_pred):\n",
    "    # Prevent log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = autograd_np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # CE = -summation{ (y_i * log(y_pred_i) + (1-y_i)*log(1-y_pred_i)}\n",
    "    return (1/y.shape[0]) * -autograd_np.sum((y * autograd_np.log(y_pred)) + ((1-y) * autograd_np.log(1-y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaec768",
   "metadata": {},
   "source": [
    "## The Dense Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dfbab",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture implements three dense (fully-connected) layers after feature extraction. This class defines a layer and supports weight and bias management - both during initialization as well as during backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e21fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    \"\"\"Class to manage a 'dense' or fully-connected layer of neurons.\"\"\"\n",
    "    def __init__(self, layer_config):\n",
    "        \"\"\"\n",
    "        Initialize a 'layer' of a neural network.\n",
    "\n",
    "        Args:\n",
    "            layer_config: A dict containing configuration information. Helps configure the following:\n",
    "                W: a vector of weights  to match mathematical convention of W.Tx + b\n",
    "                b: a vector of biases b for each neuron\n",
    "                z: a vector of preactivation values (i.e. weighted sum input to each neuron)\n",
    "                activation_function: An autograd-tracked mathematical function to add nonlinearity to the preactivation z\n",
    "                a: a vector of postactivation values (i.e. activation_function(z))\n",
    "                prev_z: a vector of preactivation values of the previous layer\n",
    "                prev_a: a vector of postactivation values of the previous layer\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.input_size = layer_config['input_dim']\n",
    "        self.output_size = layer_config['output_dim']\n",
    "        self.id = layer_config.get('id', None)\n",
    "\n",
    "        # For random normal, mu=0, sigma=1\n",
    "        mu, sigma = 0, 0.01\n",
    "        self.W = autograd_np.random.normal(loc=mu, scale=sigma, size=(self.output_size, self.input_size))\n",
    "        self.b =  autograd_np.zeros(self.output_size)\n",
    "        activation_function = 'relu' if not layer_config.get('activation') else layer_config.get('activation')\n",
    "        self.activation = activation_function_map[activation_function]\n",
    "        \n",
    "        # Current Layer: pre-activation and output values\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "        # Previous Layer: pre-activation output values for backprop\n",
    "        self.prev_layer_z = None\n",
    "        self.prev_layer_a = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Implement the forward pass.\n",
    "\n",
    "        Args:\n",
    "            input: A previous FC Layer or the raw input x values. This will be used to compute z and a, and keeps track of prev_z and prev_a\n",
    "                for use during backpropagation.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if isinstance(input, FullyConnectedLayer):\n",
    "            x = input.a # input is the previous FC layer, not raw X values, so extract the post-activation vector a from the FC layer\n",
    "            self.prev_layer_z = input.z\n",
    "            self.prev_layer_a = input.a   \n",
    "        else:\n",
    "            x = input # input is the raw X values\n",
    "            self.prev_layer_z = None\n",
    "            self.prev_layer_a = input\n",
    "\n",
    "        # Ensure we do not anger the numpy gods\n",
    "        assert x.shape[0] == self.W.shape[1], f\"Shape mismatch: input dim {x.shape[0]}, W dim {self.W.shape[1]}\"\n",
    "\n",
    "        # Computation 1: z[l] = W[l]·x + b[l] , where x = a[l-1] if the input is a FC layer, and the raw X values otherwise\n",
    "        self.z = autograd_np.dot(self.W, x) + self.b\n",
    "        # Computation 2: a[l] = activation(z[l])\n",
    "        self.a = self.activation(self.z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            dA: Gradient of loss L wrt. the output (activation) of this layer. Shorthand for to dL/da\n",
    "        Returns:\n",
    "            dA[l-1]: Gradient of loss L wrt. the output of the previous layer, for backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute error signal dL/dz[l] called del[l] = dL/da[l] x σ'(z[l]), where σ' = activation derivative\n",
    "        # del[l] = dL/da[l] x σ'(z[l]) \n",
    "        activation_derivative = elementwise_grad(self.activation) # σ'()\n",
    "        self.del_l = dA * activation_derivative(self.z) # dL/da[l] x σ'(z[l]) \n",
    "\n",
    "        # Step 2: Compute loss grad wrt. W and b of current layer i.e. dL/dW[l] and dL/db[l]\n",
    "        # dL/dw i.e. dW = del[l] x a[l-1].T\n",
    "        # dL/db i.e. db = del[l]\n",
    "        self.dW = autograd_np.outer(self.del_l, self.prev_layer_a)\n",
    "        self.db = self.del_l\n",
    "\n",
    "        # Step 3: Propagate dL/da[l-1] backwards to use in the del[l] calculation (Step 1) in the previous layer\n",
    "        # del[l-1]  = W[l].T * del[l] * σ'(z[l-1])\n",
    "\n",
    "        if self.prev_layer_z is not None:\n",
    "            del_l_prev = autograd_np.dot(self.W.T, self.del_l) * activation_derivative(self.prev_layer_z)\n",
    "        else:\n",
    "            del_l_prev = autograd_np.dot(self.W.T, self.del_l)\n",
    "        \n",
    "        return del_l_prev\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        # θ <- θ - η · ∇L\n",
    "        self.W = self.W - learning_rate * self.dW\n",
    "        self.b = self.b - learning_rate * self.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5061fb4",
   "metadata": {},
   "source": [
    "## Implementing LeNet-5 (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a124ba",
   "metadata": {},
   "source": [
    "### Gathering Data: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c7065",
   "metadata": {},
   "source": [
    "LeNet-5 was trained on actual handwritten digits extracted from material belonging to the US Postal Service; fortunately, as the dataset is foundational andd quite popular, it is available to download and parse easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db607ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc552db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x117fc6990>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X.values[0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245277",
   "metadata": {},
   "source": [
    "### The first forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079dd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class label for sample 42: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted class label probabilities for sample 42: \n",
      "[0.498162   0.49975479 0.50428915 0.5008841  0.50023381 0.49924943\n",
      " 0.50822488 0.50474022 0.49720043 0.50644706]\n",
      "\n",
      "CrossEntropy Loss for first forward pass: 0.695121025150937\n"
     ]
    }
   ],
   "source": [
    "# LeNet-5 (only dense layers for testing)\n",
    "input_layer =  FullyConnectedLayer(layer_config={'input_dim': 784,'output_dim': 30, 'activation': 'relu', 'id': 'input'})\n",
    "hidden_layer = FullyConnectedLayer(layer_config={'input_dim': 30, 'output_dim': 10, 'activation': 'relu', 'id': 'hidden'})\n",
    "output_layer = FullyConnectedLayer(layer_config={'input_dim': 10, 'output_dim': 10, 'activation': 'sigmoid', 'id': 'output'})\n",
    "\n",
    "# Pick a sample from the training data and dump it directly into the first dense layer (input_layer).\n",
    "# NOTE: for learning purposes only - eventually, there will be additional layers behind the dense layers \n",
    "# and so the output of those layers will be funneled into the first dense layer instead.\n",
    "sample_num = 42\n",
    "x = X.values[sample_num].copy()\n",
    "\n",
    "# Forward pass through all layers\n",
    "input_layer.forward(input=x)\n",
    "hidden_layer.forward(input_layer)\n",
    "output_layer.forward(hidden_layer)\n",
    "\n",
    "# Compare prediction vs ground truth\n",
    "y_true = autograd_np.eye(10)[int(y.values[sample_num])] \n",
    "y_pred = output_layer.a\n",
    "loss = _crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"Actual class label for sample {sample_num}: \\n{y_true}\")\n",
    "print(f\"Predicted class label probabilities for sample {sample_num}: \\n{y_pred}\")\n",
    "print(f\"\\nCrossEntropy Loss for first forward pass: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71d34",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "We now have a loss value that compares the actual class label (which will only have a single 1 in the one-hot-encoded vector of size $n$, where $n$ is the number of classes). The next step is, naturally, to adjust weights and biases so the error is not so high next time - but how? We need some way to understand:\n",
    "\n",
    "1. How the loss $L$ changes given a change in the network output $a^{[l]}$: $\\frac{dL}{da^{[l]}}$\n",
    "2. How a change in the weights $W$ or biases $b$ affects this $a^{[l]}$ value, and therefore the loss $L$: $\\frac{dL}{dW}$ and $\\frac{dL}{db}$. \n",
    "\n",
    "How do we do this? **Calculus!** \n",
    "\n",
    "As we enumerated above in point #2, we now need to evaluate the two terms $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ and use the chain rule to see how $\\frac{dL}{da^{[l]}}$ is affected.\n",
    "\n",
    "1. \n",
    "    $\\frac{dL}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{dW}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{dW} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{dW} = a^{[1]}$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{dW} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times a^{[1]}$\n",
    "\n",
    "2. \n",
    "    $\\frac{dL}{db}\n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{db} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{db}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{db} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{db} = 1$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{db} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}}$\n",
    "\n",
    "<br />\n",
    "\n",
    "The term $\\frac{dL}{da^{[l]}} \\times \\frac{da^{[2]}}{dz^{[l]}}$ simplifies to $\\frac{dL}{dz^{[l]}}$ and is called the *error signal* $\\delta^{[l]}.$ Using this info, we can rewrite both expressions as:\n",
    "\n",
    "$\\frac{dL}{dW} = \\delta^{[2]} \\times a^{[1]}$, and \n",
    "$\\frac{dL}{db} = \\delta^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c99e3",
   "metadata": {},
   "source": [
    "### The Loss Gradient and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067a02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: [ 7.35202919e-04  9.80857942e-05 -1.71570234e-03 -3.53639177e-04\n",
      " -9.35221962e-05  3.00226611e-04 -3.29024739e-03 -1.89614312e-03\n",
      "  1.11984014e-03 -2.57896876e-03]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of loss wrt model output, i.e. dL/da[2]\n",
    "loss_grad = grad(_crossentropy)\n",
    "gradients = loss_grad(y_true, y_pred)\n",
    "print(f\"grad: {gradients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3da81a",
   "metadata": {},
   "source": [
    "Okay great! Now that we know the math, and have computed $\\frac{dL}{da^{[l]}}$ in the cell above, it's time to implement it as code. We'll start with the error signal $\\delta^{[l]}$.\n",
    "\n",
    "#### The error signal $\\delta^{[l]}$\n",
    "Now, to compute these terms in the backward() method of a layer $l$, we first compute the value of the $\\delta^{[l]}$ vector (in this network, backprop starts from $l=2$):\n",
    " \n",
    "$\\delta^{[2]} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} = gradients \\times \\sigma'(z^{[2]})$, where $\\sigma$ is the activation function in layer $2$ and $\\sigma'$ is its derivative.\n",
    "\n",
    "As code, ```del_l = dA * activation_derivative(self.z)```\n",
    "\n",
    "Next, we use this $\\delta^{[2]}$ term in the calculations for $\\frac{dL}{dW} \\ and \\ \\frac{dL}{db}:$\n",
    "\n",
    "$\\frac{dL}{dW} = \\delta^{[2]} \\times a^{[1]}, \\ \\frac{dL}{db} = \\delta^{[2]}$.\n",
    "\n",
    "As code:\n",
    "```dW = np.outer(del_l, self.prev_layer_a)```, ```db = np.outer(del_l, self.prev_layer_a)```\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Backpropagating the error signal\n",
    "\n",
    "When we started the backprop process, we looked at how the loss $L$ changes given a change in the network output $a^{[l]}$. i.e.: $\\frac{dL}{da^{[l]}}$.\n",
    "\n",
    "This framework holds true for the previous layer as well. From its perspective, it is *also* recieving a loss signal, but unlike the final layer, it is not the *\"gradient of loss wrt network output\"* but rather the *\"gradient of loss wrt **previous layer output\"***: $\\frac{dL}{da^{[l-1]}}$.\n",
    "\n",
    "This is the core of backprop: at each layer $l$, we compute $\\frac{dL}{da^{[l-1]}}$ and propagate it back to the previous layer $l-1$ to be used in the $\\delta^{[l-1]}$ calculation of that $l-1$ layer. Using the chain rule,\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\frac{dL}{dz^{[2]}} \\times \\frac{dz^{[2]}}{da^{[1]}}$. We already know $\\frac{dL}{dz^{[2]}} = \\delta^{[2]}$, so\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\delta^{[2]} \\times \\frac{dz^{[2]}}{da^{[1]}} = \\delta^{[2]} \\times \\frac{d(W^{[2]}a^{[1]})}{da^{[1]}}$. Taking the derivative of that final term,\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\delta^{[2]} \\times W^{[2]}.$ \n",
    "\n",
    "This is straightforward to implement in the backward() method of layer $l$:\n",
    "\n",
    "```np.dot(self.W.T, del_l) * activation_derivative(self.prev_layer_z)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dad45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate!\n",
    "dA = gradients\n",
    "dA_2 = output_layer.backward(dA)\n",
    "dA_1 = hidden_layer.backward(dA_2)\n",
    "dA_0 = input_layer.backward(dA_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb4a7c",
   "metadata": {},
   "source": [
    "### A step in the right(?) direction\n",
    "\n",
    "Now that the gradients $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ have been computed across all layers, it is finally time to fine-tune the weights and biases accordingly. Our math above tells us that the weight vector $W$ and the bias vector $b$ eventually influence the loss L, so nudging them slighly according to the computed gradients *should* reduce the loss.\n",
    "\n",
    "I say *should*, because there can be local minima in the loss surface that the gradients naturally pick up on.\n",
    "\n",
    "#### Adjusting weights $W^{[l]}$ and biases $b^{[l]}$\n",
    "Now that we have $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ for layer $l$, we can adjust the weights $W^{[l]}$ and biases $b^{[l]}$ for layer $l$. \n",
    "\n",
    "These gradients wrt. to the loss represents the direction of steepest increase in the loss surface - essentially, it represents the direction to maximally *increase* loss. We want the opposite of this (maximally *decrease* loss), so  we perform the 'nudge', or *step* in the *opposite* direction of the gradient by using a $-$ sign:\n",
    "\n",
    "$$W = W - \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\frac{dL}{db}$$\n",
    "\n",
    "However, since the error terms are quite high in magnitude especially at the start of training, directly stepping in a direction will produce a huge change in the parameters. To control this, we introduce a learning rate $\\eta$, which is used as a reduction/control factor in the step:\n",
    "\n",
    "$$W = W - \\eta \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\eta \\frac{dL}{db}$$\n",
    "\n",
    "In deep learning math convention, all trainable params (so the weights and biases) are represented by $\\theta$, and the derivatove of the loss wrt to these paramaters is represented by $\\nabla_{\\theta}L$. The optimizing step can therefore be written in a single expression as:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}L$$\n",
    "\n",
    "\n",
    "As code, \n",
    "\n",
    "```self.W = self.W - learning_rate * self.dW```\n",
    "\n",
    "```self.b = self.b - learning_rate * self.b```\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "output_layer.step(learning_rate=learning_rate)\n",
    "hidden_layer.step(learning_rate=learning_rate)\n",
    "input_layer.step(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb253c",
   "metadata": {},
   "source": [
    "## Testing FC layers out a smaller dataset before MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bdec8",
   "metadata": {},
   "source": [
    "LeNet-5 requires additional types of layers such as Convolution and MaxPooling, so to ensure the dense layers actually work before they are added in, we first test the FC layers on a small numerical dataset. I've used the [defaults from Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) here since it seems to work well [in practice](https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0800378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "input_layer = FullyConnectedLayer({'input_dim': 4, 'output_dim': 100, 'activation': 'relu'})\n",
    "hidden_layer_1 = FullyConnectedLayer({'input_dim': 100, 'output_dim': 100, 'activation': 'relu'})\n",
    "hidden_layer_2 = FullyConnectedLayer({'input_dim': 100, 'output_dim': 100, 'activation': 'relu'})\n",
    "output_layer = FullyConnectedLayer({'input_dim': 100, 'output_dim': 3, 'activation': 'sigmoid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8d7830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6883\n",
      "Epoch 2, Loss: 0.6792\n",
      "Epoch 3, Loss: 0.6717\n",
      "Epoch 4, Loss: 0.6655\n",
      "Epoch 5, Loss: 0.6605\n",
      "Epoch 6, Loss: 0.6563\n",
      "Epoch 7, Loss: 0.6529\n",
      "Epoch 8, Loss: 0.6501\n",
      "Epoch 9, Loss: 0.6478\n",
      "Epoch 10, Loss: 0.6459\n",
      "Epoch 11, Loss: 0.6444\n",
      "Epoch 12, Loss: 0.6431\n",
      "Epoch 13, Loss: 0.6420\n",
      "Epoch 14, Loss: 0.6411\n",
      "Epoch 15, Loss: 0.6404\n",
      "Epoch 16, Loss: 0.6398\n",
      "Epoch 17, Loss: 0.6393\n",
      "Epoch 18, Loss: 0.6389\n",
      "Epoch 19, Loss: 0.6385\n",
      "Epoch 20, Loss: 0.6382\n",
      "Epoch 21, Loss: 0.6380\n",
      "Epoch 22, Loss: 0.6378\n",
      "Epoch 23, Loss: 0.6376\n",
      "Epoch 24, Loss: 0.6375\n",
      "Epoch 25, Loss: 0.6374\n",
      "Epoch 26, Loss: 0.6373\n",
      "Epoch 27, Loss: 0.6372\n",
      "Epoch 28, Loss: 0.6371\n",
      "Epoch 29, Loss: 0.6371\n",
      "Epoch 30, Loss: 0.6370\n",
      "Epoch 31, Loss: 0.6370\n",
      "Epoch 32, Loss: 0.6369\n",
      "Epoch 33, Loss: 0.6369\n",
      "Epoch 34, Loss: 0.6369\n",
      "Epoch 35, Loss: 0.6369\n",
      "Epoch 36, Loss: 0.6369\n",
      "Epoch 37, Loss: 0.6368\n",
      "Epoch 38, Loss: 0.6368\n",
      "Epoch 39, Loss: 0.6368\n",
      "Epoch 40, Loss: 0.6368\n",
      "Epoch 41, Loss: 0.6368\n",
      "Epoch 42, Loss: 0.6368\n",
      "Epoch 43, Loss: 0.6368\n",
      "Epoch 44, Loss: 0.6368\n",
      "Epoch 45, Loss: 0.6368\n",
      "Epoch 46, Loss: 0.6368\n",
      "Epoch 47, Loss: 0.6368\n",
      "Epoch 48, Loss: 0.6368\n",
      "Epoch 49, Loss: 0.6368\n",
      "Epoch 50, Loss: 0.6368\n",
      "Epoch 51, Loss: 0.6368\n",
      "Epoch 52, Loss: 0.6368\n",
      "Epoch 53, Loss: 0.6368\n",
      "Epoch 54, Loss: 0.6368\n",
      "Epoch 55, Loss: 0.6368\n",
      "Epoch 56, Loss: 0.6368\n",
      "Epoch 57, Loss: 0.6368\n",
      "Epoch 58, Loss: 0.6368\n",
      "Epoch 59, Loss: 0.6368\n",
      "Epoch 60, Loss: 0.6368\n",
      "Epoch 61, Loss: 0.6368\n",
      "Epoch 62, Loss: 0.6368\n",
      "Epoch 63, Loss: 0.6368\n",
      "Epoch 64, Loss: 0.6368\n",
      "Epoch 65, Loss: 0.6368\n",
      "Epoch 66, Loss: 0.6368\n",
      "Epoch 67, Loss: 0.6368\n",
      "Epoch 68, Loss: 0.6368\n",
      "Epoch 69, Loss: 0.6368\n",
      "Epoch 70, Loss: 0.6368\n",
      "Epoch 71, Loss: 0.6368\n",
      "Epoch 72, Loss: 0.6368\n",
      "Epoch 73, Loss: 0.6368\n",
      "Epoch 74, Loss: 0.6368\n",
      "Epoch 75, Loss: 0.6368\n",
      "Epoch 76, Loss: 0.6368\n",
      "Epoch 77, Loss: 0.6368\n",
      "Epoch 78, Loss: 0.6368\n",
      "Epoch 79, Loss: 0.6368\n",
      "Epoch 80, Loss: 0.6368\n",
      "Epoch 81, Loss: 0.6368\n",
      "Epoch 82, Loss: 0.6368\n",
      "Epoch 83, Loss: 0.6368\n",
      "Epoch 84, Loss: 0.6368\n",
      "Epoch 85, Loss: 0.6368\n",
      "Epoch 86, Loss: 0.6368\n",
      "Epoch 87, Loss: 0.6368\n",
      "Epoch 88, Loss: 0.6368\n",
      "Epoch 89, Loss: 0.6368\n",
      "Epoch 90, Loss: 0.6368\n",
      "Epoch 91, Loss: 0.6368\n",
      "Epoch 92, Loss: 0.6368\n",
      "Epoch 93, Loss: 0.6368\n",
      "Epoch 94, Loss: 0.6368\n",
      "Epoch 95, Loss: 0.6368\n",
      "Epoch 96, Loss: 0.6368\n",
      "Epoch 97, Loss: 0.6368\n",
      "Epoch 98, Loss: 0.6368\n",
      "Epoch 99, Loss: 0.6368\n",
      "Epoch 100, Loss: 0.6368\n"
     ]
    }
   ],
   "source": [
    "# Training params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "loss_curve = []\n",
    "\n",
    "def model_loss_on_output(y_pred, y_true):\n",
    "    return _crossentropy(y_true, y_pred)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        x = X_train[i]\n",
    "        y_true = y_train[i]\n",
    "\n",
    "        # forward pass\n",
    "        input_layer.forward(x)\n",
    "        hidden_layer_1.forward(input_layer)\n",
    "        hidden_layer_2.forward(hidden_layer_1)\n",
    "        output_layer.forward(hidden_layer_2)\n",
    "        y_pred = output_layer.a\n",
    "\n",
    "        # loss computation\n",
    "        loss_val = _crossentropy(y_true, y_pred)\n",
    "        epoch_loss += loss_val\n",
    "\n",
    "        # gradients dL/dA\n",
    "        grad_fn = grad(model_loss_on_output)\n",
    "        dA = grad_fn(y_pred, y_true)\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        dA_3 = output_layer.backward(dA)\n",
    "        dA_2 = hidden_layer_2.backward(dA_3)\n",
    "        dA_1 = hidden_layer_1.backward(dA_2)\n",
    "        dA_0 = input_layer.backward(dA_1)\n",
    "\n",
    "        # 5. Update weights\n",
    "        output_layer.step(learning_rate)\n",
    "        hidden_layer_2.step(learning_rate)\n",
    "        hidden_layer_1.step(learning_rate)\n",
    "        input_layer.step(learning_rate)\n",
    "\n",
    "    # Track and print average loss for epoch\n",
    "    avg_loss = epoch_loss / X_train.shape[0]\n",
    "    loss_curve.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b2fc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121c377d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs),loss_curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
