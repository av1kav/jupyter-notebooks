{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a66d93",
   "metadata": {},
   "source": [
    "# Neural Nets from scratch, in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce91e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, elementwise_grad, jacobian, numpy as autograd_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a923e5f",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd63c0",
   "metadata": {},
   "source": [
    "LeCun et. al.'s seminal LeNet-5 is a great basic network to grok the implementation of the math behind neural nets. To begin with, we define a set of activation functions that add non-linearity to what is otherwise essentially a complex linear regression solver:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f323",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515a1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relu(x):\n",
    "    return autograd_np.maximum(0, x)\n",
    "\n",
    "def _sigmoid(x):\n",
    "    x_clip = autograd_np.clip(x, -700, 700)\n",
    "    return 1 / (1 + autograd_np.exp(-x_clip))\n",
    "\n",
    "activation_function_map = {\n",
    "    'relu': _relu,\n",
    "    'sigmoid': _sigmoid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896ac5d",
   "metadata": {},
   "source": [
    "Next, we define loss functions to measure the quality of the output layer. Since this is a classification problem, we need class membership probabilities; the sigmoid function is used as the activation of the final layer for this. The loss function, therefore, must compare this class membership probability vector against the label (ground truth) for that training observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5f157",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9c5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(y, y_pred):\n",
    "    # RMSE = sqrt of (1/n) summation{ (y_i - y_pred_i)**2 }\n",
    "    return autograd_np.sqrt((1/y.shape[0]) * np.sum((y - y_pred) ** 2))\n",
    "\n",
    "def _crossentropy(y, y_pred):\n",
    "    # Prevent log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = autograd_np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # CE = -summation{ (y_i * log(y_pred_i) + (1-y_i)*log(1-y_pred_i)}\n",
    "    return (1/y.shape[0]) * -autograd_np.sum((y * autograd_np.log(y_pred)) + ((1-y) * autograd_np.log(1-y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaec768",
   "metadata": {},
   "source": [
    "## The Dense Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dfbab",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture implements three dense (fully-connected) layers after feature extraction. This class defines a layer and supports weight and bias management - both during initialization as well as during backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e21fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    def __init__(self, layer_config):\n",
    "        self.input_size = layer_config['input_dim']\n",
    "        self.output_size = layer_config['output_dim']\n",
    "\n",
    "        # For random normal, mu=0, sigma=1\n",
    "        mu, sigma = 0, 1\n",
    "        self.W = np.random.normal(loc=mu, scale=sigma, size=(self.output_size, self.input_size))\n",
    "        self.b =  np.random.normal(loc=mu, scale=sigma, size=self.output_size)\n",
    "        activation_function = 'relu' if not layer_config.get('activation') else layer_config.get('activation')\n",
    "        self.activation = activation_function_map[activation_function]\n",
    "        \n",
    "        # Current Layer: pre-activation and output values\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "        # Previous Layer: pre-activation output values for backprop\n",
    "        self.prev_layer_z = None\n",
    "        self.prev_layer_a = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Ensure we do not anger the numpy gods\n",
    "        assert input.shape[0] == self.W.shape[1], f\"Shape mismatch: input dim {input.shape[0]} but W expects {self.W.shape[1]}\"\n",
    "\n",
    "        # Computation 1: z[l] = W[l] a · [l-1] + b[l] \n",
    "        self.z = np.dot(self.W, input) + self.b\n",
    "        # Computation 2: a[l] = activation(z[l])\n",
    "        self.a = self.activation(self.z)\n",
    "\n",
    "        try:\n",
    "            # Keeping track of prev layer output for backprop\n",
    "            self.prev_layer_z = input.z\n",
    "            self.prev_layer_a = input.a\n",
    "        except AttributeError as e:\n",
    "            # In case the prev layer is the raw input and not another FC Layer\n",
    "            #print(f\"Gracefully handling: {e}\")\n",
    "            self.prev_layer_z = None\n",
    "            self.prev_layer_a = input\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            dA: Gradient of loss L wrt. the output (activation) of this layer. Shorthand for to dL/da\n",
    "        Returns:\n",
    "            dA[l-1]: Gradient of loss L wrt. the output of the previous layer, for backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute error signal (dL/dz[l]) called del[l] = dL/da[l] x σ'(z[l]), where σ' = activation derivative\n",
    "        # del[l] = dL/da[l] x σ'(z[l]) \n",
    "        activation_derivative = elementwise_grad(self.activation)\n",
    "        del_l = dA * activation_derivative(self.z)\n",
    "\n",
    "        # Step 2: Compute loss grad wrt. W and b of current layer i.e. dL/dW[l] and dL/db[l]\n",
    "        # dL/dw i.e. dW = del[l] x a[l-1].T\n",
    "        # dL/db i.e. db = del[l]\n",
    "        dW = np.outer(del_l, self.prev_layer_a)\n",
    "        db = del_l\n",
    "\n",
    "        # Step 3: Propagate dL/da[l-1] backwards to use in the del[l] calculation (Step 1) in the previous layer\n",
    "        # del[l-1]  = W[l].T * del[l] * σ'(z[l-1])\n",
    "        if self.prev_layer_z is not None:\n",
    "            del_l_prev = np.dot(self.W.T, del_l) * activation_derivative(self.prev_layer_z)\n",
    "        else:\n",
    "            del_l_prev = np.dot(self.W.T, del_l)\n",
    "\n",
    "        return del_l_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a124ba",
   "metadata": {},
   "source": [
    "## Gathering Data: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c7065",
   "metadata": {},
   "source": [
    "LeNet-5 was trained on actual handwritten digits extracted from material belonging to the US Postal Service; fortunately, as the dataset is foundation ald quite popular, it is available to download and parse easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db607ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc552db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x165f12450>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X.values[0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5061fb4",
   "metadata": {},
   "source": [
    "## Implementing LeNet-5 (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245277",
   "metadata": {},
   "source": [
    "### 1. The first forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079dd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class label for sample 42: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted class label probabilities for sample 42 (rounded): \n",
      "[1.    1.    1.    1.    1.    1.    0.    1.    0.311 1.   ]\n",
      "\n",
      "CrossEntropy Loss for first forward pass: 24.214924232533306\n"
     ]
    }
   ],
   "source": [
    "# LeNet-5 (only dense layers for testing)\n",
    "input_layer =  FullyConnectedLayer(layer_config={'input_dim': 784,'output_dim': 30, 'activation': 'relu'})\n",
    "hidden_layer = FullyConnectedLayer(layer_config={'input_dim': 30, 'output_dim': 10, 'activation': 'relu'})\n",
    "output_layer = FullyConnectedLayer(layer_config={'input_dim': 10, 'output_dim': 10, 'activation': 'sigmoid'})\n",
    "\n",
    "# Pick a sample from the training data and dump it directly into the first dense layer (input_layer).\n",
    "# NOTE: for learning purposes only - eventually, there will be additional layers behind the dense layers \n",
    "# and so the output of those layers will be funneled into the first dense layer instead.\n",
    "sample_num = 42\n",
    "x = X.values[sample_num].copy()\n",
    "\n",
    "# Forward pass through all layers\n",
    "input_layer.forward(input=x)\n",
    "hidden_layer.forward(input_layer.a)\n",
    "output_layer.forward(hidden_layer.a)\n",
    "\n",
    "# Compare prediction vs ground truth\n",
    "y_true = np.eye(10)[int(y.values[sample_num])] \n",
    "y_pred = output_layer.a\n",
    "loss = _crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"Actual class label for sample {sample_num}: \\n{y_true}\")\n",
    "print(f\"Predicted class label probabilities for sample {sample_num} (rounded): \\n{np.round(y_pred, decimals=3)}\")\n",
    "print(f\"\\nCrossEntropy Loss for first forward pass: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71d34",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "We now have a loss value that compares the actual class label (which will only have a single 1 in the one-hot-encoded vector of size $n$, where $n$ is the number of classes). The next step is, naturally, to adjust weights and biases - but how? For this, we need to understand how the Loss $L$ changes given a change in the weights $W$ or biases $b$. How do we do this? Calculus!\n",
    "\n",
    "We now need to evaluate these two terms: $\\frac{dL}{dW}$ and $\\frac{dL}{db}$\n",
    "\n",
    "1. $\\frac{dL}{dW}$\n",
    "\n",
    "    $\\frac{dL}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{dW}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{dW} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{dW} = a^{[1]}$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{dW} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times a^{[1]}$\n",
    "\n",
    "\n",
    "\n",
    "2. $\\frac{dL}{db}$\n",
    "\n",
    "    $\\frac{dL}{db}\n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{db} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{db}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{db} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{db} = 1$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{db} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}}$\n",
    "\n",
    "\n",
    "The term $\\frac{dL}{da^{[l]}} \\times \\frac{da^{[2]}}{dz^{[l]}}$ simplifies to $\\frac{dL}{dz^{[l]}}$ and is called the *error signal* $\\delta^{[l]}.$ Using this info, we can rewrite both expressions as:\n",
    "\n",
    "$\\frac{dL}{dW} = \\delta^{[2]} \\times a^{[1]}$, and \n",
    "$\\frac{dL}{db} = \\delta^{[2]}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "067a02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: [-3.4539576  -3.4539576  -3.4539576  -3.4539576  -3.4539576  -3.4539576\n",
      "  3.45387764 -3.4539576   0.0796425  -3.4539576 ]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of loss wrt model output, i.e. dL/da[2]\n",
    "loss_grad = grad(_crossentropy)\n",
    "gradients = loss_grad(y_true, y_pred)\n",
    "print(f\"grad: {gradients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dad45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate!\n",
    "dA_2 = gradients\n",
    "dA_1 = output_layer.backward(dA_2)\n",
    "dA_0 = hidden_layer.backward(dA_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d83509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
