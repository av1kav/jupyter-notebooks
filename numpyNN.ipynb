{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a66d93",
   "metadata": {},
   "source": [
    "# Neural Nets from scratch, in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce91e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, elementwise_grad, jacobian, numpy as autograd_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a923e5f",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd63c0",
   "metadata": {},
   "source": [
    "LeCun et. al.'s seminal LeNet-5 is a great basic network to grok the implementation of the math behind neural nets. To begin with, we define a set of activation functions that add non-linearity to what is otherwise essentially a complex linear regression solver:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f323",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "515a1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relu(x):\n",
    "    return autograd_np.maximum(0, x)\n",
    "\n",
    "def _sigmoid(x):\n",
    "    x_clip = autograd_np.clip(x, -700, 700)\n",
    "    return 1 / (1 + autograd_np.exp(-x_clip))\n",
    "\n",
    "def _softmax(x):\n",
    "    # Subtract max for numerical stability (avoids overflow from exp)\n",
    "    shifted_x = x - autograd_np.max(x)\n",
    "    exps = autograd_np.exp(shifted_x)\n",
    "    return exps / autograd_np.sum(exps)\n",
    "\n",
    "activation_function_map = {\n",
    "    'relu': _relu,\n",
    "    'sigmoid': _sigmoid,\n",
    "    'softmax': _softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896ac5d",
   "metadata": {},
   "source": [
    "Next, we define loss functions to measure the quality of the output layer. Since this is a classification problem, we need class membership probabilities; the sigmoid function is used as the activation of the final layer for this. The loss function, therefore, must compare this class membership probability vector against the label (ground truth) for that training observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5f157",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d9c5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(y, y_pred):\n",
    "    # RMSE = sqrt of (1/n) summation{ (y_i - y_pred_i)**2 }\n",
    "    return autograd_np.sqrt((1/y.shape[0]) * np.sum((y - y_pred) ** 2))\n",
    "\n",
    "def _crossentropy(y, y_pred):\n",
    "    # Prevent log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = autograd_np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # CE = -summation{ (y_i * log(y_pred_i) + (1-y_i)*log(1-y_pred_i)}\n",
    "    return (1/y.shape[0]) * -autograd_np.sum((y * autograd_np.log(y_pred)) + ((1-y) * autograd_np.log(1-y_pred)))\n",
    "\n",
    "def _softmax_crossentropy(y_true, y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = autograd_np.clip(y_pred, eps, 1 - eps)\n",
    "    return -autograd_np.sum(y_true * autograd_np.log(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaec768",
   "metadata": {},
   "source": [
    "## The Dense Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dfbab",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture implements three dense (fully-connected) layers after feature extraction. This class defines a layer and supports weight and bias management - both during initialization as well as during backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3e21fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    def __init__(self, layer_config):\n",
    "        self.input_size = layer_config['input_dim']\n",
    "        self.output_size = layer_config['output_dim']\n",
    "        self.id = layer_config.get('id', None)\n",
    "\n",
    "        # For random normal, mu=0, sigma=1\n",
    "        mu, sigma = 0, 1\n",
    "        self.W = np.random.normal(loc=mu, scale=sigma, size=(self.output_size, self.input_size))\n",
    "        self.b =  np.random.normal(loc=mu, scale=sigma, size=self.output_size)\n",
    "        activation_function = 'relu' if not layer_config.get('activation') else layer_config.get('activation')\n",
    "        self.activation = activation_function_map[activation_function]\n",
    "        \n",
    "        # Current Layer: pre-activation and output values\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "        # Previous Layer: pre-activation output values for backprop\n",
    "        self.prev_layer_z = None\n",
    "        self.prev_layer_a = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Implement the forward pass.\n",
    "\n",
    "        Args:\n",
    "            input: A previous FC Layer or the raw input x values. This will be used to compute z and a, and keeps track of prev_z and prev_a\n",
    "                for use during backpropagation.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if isinstance(input, FullyConnectedLayer):\n",
    "            x = input.a # input is the previous FC layer, not raw values, so extract the post-activation vector a\n",
    "            self.prev_layer_z = input.z\n",
    "            self.prev_layer_a = input.a   \n",
    "        else:\n",
    "            x = input # input is the raw X values\n",
    "            self.prev_layer_z = None\n",
    "            self.prev_layer_a = input\n",
    "\n",
    "        # Ensure we do not anger the numpy gods\n",
    "        assert x.shape[0] == self.W.shape[1], f\"Shape mismatch: input dim {x.shape[0]} but W expects {self.W.shape[1]}\"\n",
    "\n",
    "        # Computation 1: z[l] = W[l]·x + b[l] (x = a[l-1] if the input is a FC layer, otherwise x=x) \n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        # Computation 2: a[l] = activation(z[l])\n",
    "        self.a = self.activation(self.z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            dA: Gradient of loss L wrt. the output (activation) of this layer. Shorthand for to dL/da\n",
    "        Returns:\n",
    "            dA[l-1]: Gradient of loss L wrt. the output of the previous layer, for backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute error signal dL/dz[l] called del[l] = dL/da[l] x σ'(z[l]), where σ' = activation derivative\n",
    "        # del[l] = dL/da[l] x σ'(z[l]) \n",
    "        activation_derivative = elementwise_grad(self.activation) # σ'()\n",
    "        self.del_l = dA * activation_derivative(self.z) # dL/da[l] x σ'(z[l]) \n",
    "\n",
    "        # override\n",
    "        if self.activation.__name__ == '_softmax':\n",
    "            self.del_l = dA  # because dL/dz = a - y\n",
    "\n",
    "        # Step 2: Compute loss grad wrt. W and b of current layer i.e. dL/dW[l] and dL/db[l]\n",
    "        # dL/dw i.e. dW = del[l] x a[l-1].T\n",
    "        # dL/db i.e. db = del[l]\n",
    "        self.dW = np.outer(self.del_l, self.prev_layer_a)\n",
    "        self.db = self.del_l\n",
    "\n",
    "        # Step 3: Propagate dL/da[l-1] backwards to use in the del[l] calculation (Step 1) in the previous layer\n",
    "        # del[l-1]  = W[l].T * del[l] * σ'(z[l-1])\n",
    "\n",
    "        if self.prev_layer_z is not None:\n",
    "            del_l_prev = np.dot(self.W.T, self.del_l) * activation_derivative(self.prev_layer_z)\n",
    "        else:\n",
    "            del_l_prev = np.dot(self.W.T, self.del_l)\n",
    "        \n",
    "        return del_l_prev\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        self.W = self.W - learning_rate * self.dW\n",
    "        self.b = self.b - learning_rate * self.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a124ba",
   "metadata": {},
   "source": [
    "## Gathering Data: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c7065",
   "metadata": {},
   "source": [
    "LeNet-5 was trained on actual handwritten digits extracted from material belonging to the US Postal Service; fortunately, as the dataset is foundation ald quite popular, it is available to download and parse easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db607ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc552db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x126d94290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X.values[0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5061fb4",
   "metadata": {},
   "source": [
    "## Implementing LeNet-5 (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245277",
   "metadata": {},
   "source": [
    "### 1. The first forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079dd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class label for sample 42: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted class label probabilities for sample 42 (rounded): \n",
      "[0. 1. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "CrossEntropy Loss for first forward pass: 13.815830396936352\n"
     ]
    }
   ],
   "source": [
    "# LeNet-5 (only dense layers for testing)\n",
    "input_layer =  FullyConnectedLayer(layer_config={'input_dim': 784,'output_dim': 30, 'activation': 'relu', 'id': 'input'})\n",
    "hidden_layer = FullyConnectedLayer(layer_config={'input_dim': 30, 'output_dim': 10, 'activation': 'relu', 'id': 'hidden'})\n",
    "output_layer = FullyConnectedLayer(layer_config={'input_dim': 10, 'output_dim': 10, 'activation': 'sigmoid', 'id': 'output'})\n",
    "\n",
    "# Pick a sample from the training data and dump it directly into the first dense layer (input_layer).\n",
    "# NOTE: for learning purposes only - eventually, there will be additional layers behind the dense layers \n",
    "# and so the output of those layers will be funneled into the first dense layer instead.\n",
    "sample_num = 42\n",
    "x = X.values[sample_num].copy()\n",
    "\n",
    "# Forward pass through all layers\n",
    "input_layer.forward(input=x)\n",
    "hidden_layer.forward(input_layer)\n",
    "output_layer.forward(hidden_layer)\n",
    "\n",
    "# Compare prediction vs ground truth\n",
    "y_true = np.eye(10)[int(y.values[sample_num])] \n",
    "y_pred = output_layer.a\n",
    "loss = _crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"Actual class label for sample {sample_num}: \\n{y_true}\")\n",
    "print(f\"Predicted class label probabilities for sample {sample_num} (rounded): \\n{np.round(y_pred, decimals=3)}\")\n",
    "print(f\"\\nCrossEntropy Loss for first forward pass: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71d34",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "We now have a loss value that compares the actual class label (which will only have a single 1 in the one-hot-encoded vector of size $n$, where $n$ is the number of classes). The next step is, naturally, to adjust weights and biases so the error is not so high next time - but how? We need some way to understand:\n",
    "\n",
    "1. How the loss $L$ changes given a change in the network output $a^{[l]}$: $\\frac{dL}{da^{[l]}}$\n",
    "2. How a change in the weights $W$ or biases $b$ affects this $a^{[l]}$ value, and therefore the loss $L$: $\\frac{dL}{dW}$ and $\\frac{dL}{db}$. \n",
    "\n",
    "How do we do this? **Calculus!** \n",
    "\n",
    "As we enumerated above in point #2, we now need to evaluate the two terms $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ and use the chain rule to see how $\\frac{dL}{da^{[l]}}$ is affected.\n",
    "\n",
    "1. \n",
    "    $\\frac{dL}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dW} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{dW}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{dW} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{dW} = a^{[1]}$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{dW} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times a^{[1]}$\n",
    "\n",
    "2. \n",
    "    $\\frac{dL}{db}\n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{db} \n",
    "    = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} \\times \\frac{dz^{[2]}}{db}$ \n",
    "\n",
    "    $Since \\ \\frac{dz^{[2]}}{db} =  \\frac{d(W^{[2]}a^{[1]} + b^{[2]})}{db} = 1$,\n",
    "\n",
    "    $Therefore \\ \\frac{dL}{db} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}}$\n",
    "\n",
    "<br />\n",
    "\n",
    "The term $\\frac{dL}{da^{[l]}} \\times \\frac{da^{[2]}}{dz^{[l]}}$ simplifies to $\\frac{dL}{dz^{[l]}}$ and is called the *error signal* $\\delta^{[l]}.$ Using this info, we can rewrite both expressions as:\n",
    "\n",
    "$\\frac{dL}{dW} = \\delta^{[2]} \\times a^{[1]}$, and \n",
    "$\\frac{dL}{db} = \\delta^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c99e3",
   "metadata": {},
   "source": [
    "### 2. The Loss Gradient and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067a02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: [ 3.45387764 -3.4539576  -3.4539576  -3.4539576  -3.4539576   3.45387764\n",
      "  3.45387764 -3.4539576   3.45387764  3.45387764]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of loss wrt model output, i.e. dL/da[2]\n",
    "loss_grad = grad(_crossentropy)\n",
    "gradients = loss_grad(y_true, y_pred)\n",
    "print(f\"grad: {gradients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3da81a",
   "metadata": {},
   "source": [
    "Okay great! Now that we know the math, and have computed $\\frac{dL}{da^{[l]}}$ in the cell above, it's time to implement it as code. We'll start with the error signal $\\delta^{[l]}$.\n",
    "\n",
    "#### The error signal $\\delta^{[l]}$\n",
    "Now, to compute these terms in the backward() method of a layer $l$, we first compute the value of the $\\delta^{[l]}$ vector (in this network, backprop starts from $l=2$):\n",
    " \n",
    "$\\delta^{[2]} = \\frac{dL}{da^{[2]}} \\times \\frac{da^{[2]}}{dz^{[2]}} = gradients \\times \\sigma'(z^{[2]})$, where $\\sigma$ is the activation function in layer $2$ and $\\sigma'$ is its derivative.\n",
    "\n",
    "As code, ```del_l = dA * activation_derivative(self.z)```\n",
    "\n",
    "Next, we use this $\\delta^{[2]}$ term in the calculations for $\\frac{dL}{dW} \\ and \\ \\frac{dL}{db}:$\n",
    "\n",
    "$\\frac{dL}{dW} = \\delta^{[2]} \\times a^{[1]}, \\ \\frac{dL}{db} = \\delta^{[2]}$.\n",
    "\n",
    "As code:\n",
    "```dW = np.outer(del_l, self.prev_layer_a)```, ```db = np.outer(del_l, self.prev_layer_a)```\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Backpropagating the error signal\n",
    "\n",
    "When we started the backprop process, we looked at how the loss $L$ changes given a change in the network output $a^{[l]}$. i.e.: $\\frac{dL}{da^{[l]}}$.\n",
    "\n",
    "This framework holds true for the previous layer as well. From its perspective, it is *also* recieving a loss signal, but unlike the final layer, it is not the *\"gradient of loss wrt network output\"* but rather the *\"gradient of loss wrt **previous layer output\"***: $\\frac{dL}{da^{[l-1]}}$.\n",
    "\n",
    "This is the core of backprop: at each layer $l$, we compute $\\frac{dL}{da^{[l-1]}}$ and propagate it back to the previous layer $l-1$ to be used in the $\\delta^{[l-1]}$ calculation of that $l-1$ layer. Using the chain rule,\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\frac{dL}{dz^{[2]}} \\times \\frac{dz^{[2]}}{da^{[1]}}$. We already know $\\frac{dL}{dz^{[2]}} = \\delta^{[2]}$, so\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\delta^{[2]} \\times \\frac{dz^{[2]}}{da^{[1]}} = \\delta^{[2]} \\times \\frac{d(W^{[2]}a^{[1]})}{da^{[1]}}$. Taking the derivative of that final term,\n",
    "\n",
    "$\\frac{dL}{da^{[1]}} = \\delta^{[2]} \\times W^{[2]}.$ \n",
    "\n",
    "This is straightforward to implement in the backward() method of layer $l$:\n",
    "\n",
    "```np.dot(self.W.T, del_l) * activation_derivative(self.prev_layer_z)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dad45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avenugopal/Desktop/Prep/Python/jupyter-notebooks/.venv/lib/python3.11/site-packages/autograd/numpy/numpy_vjps.py:125: RuntimeWarning: overflow encountered in square\n",
      "  lambda ans, x, y: unbroadcast_f(y, lambda g: -g * x / y**2),\n"
     ]
    }
   ],
   "source": [
    "# Backpropagate!\n",
    "dA = gradients\n",
    "dA_2 = output_layer.backward(dA)\n",
    "dA_1 = hidden_layer.backward(dA_2)\n",
    "dA_0 = input_layer.backward(dA_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb4a7c",
   "metadata": {},
   "source": [
    "### 3. A step in the right(?) direction\n",
    "\n",
    "Now that the gradients $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ have been computed across all layers, it is finally time to fine-tune the weights and biases accordingly. Our math above tells us that the weight vector $W$ and the bias vector $b$ eventually influence the loss L, so nudging them slighly according to the computed gradients *should* reduce the loss.\n",
    "\n",
    "I say *should*, because there can be local minima in the loss surface that the gradients naturally pick up on.\n",
    "\n",
    "#### Adjusting weights $W^{[l]}$ and biases $b^{[l]}$\n",
    "Now that we have $\\frac{dL}{dW}$ and $\\frac{dL}{db}$ for layer $l$, we can adjust the weights $W^{[l]}$ and biases $b^{[l]}$ for layer $l$. \n",
    "\n",
    "These gradients wrt. to the loss represents the direction of steepest increase in the loss surface - essentially, it represents the direction to maximally *increase* loss. We want the opposite of this (maximally *decrease* loss), so  we perform the 'nudge', or *step* in the *opposite* direction of the gradient by using a $-$ sign:\n",
    "\n",
    "$$W = W - \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\frac{dL}{db}$$\n",
    "\n",
    "However, since the error terms are quite high in magnitude especially at the start of training, directly stepping in a direction will produce a huge change in the parameters. To control this, we introduce a learning rate $\\eta$, which is used as a reduction/control factor in the step:\n",
    "\n",
    "$$W = W - \\eta \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\eta \\frac{dL}{db}$$\n",
    "\n",
    "In deep learning math convention, all trainable params (so the weights and biases) are represented by $\\theta$. The optimizing step can therefore be written in a single expression as:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\Delta_{\\theta}L$$\n",
    "\n",
    "\n",
    "As code, \n",
    "\n",
    "```self.W = self.W - learning_rate * self.dW```\n",
    "\n",
    "```self.b = self.b - learning_rate * self.b```\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "output_layer.step(learning_rate=learning_rate)\n",
    "hidden_layer.step(learning_rate=learning_rate)\n",
    "input_layer.step(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb253c",
   "metadata": {},
   "source": [
    "## Trying out a smaller dataset before MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3022c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||dW|| (output): 488.8002425107878\n",
      "||dW|| (output): 540.4382604032429\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214133\n",
      "||dW|| (output): 655.9871335214134\n",
      "||dW|| (output): 655.9871335214136\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.9871335214142\n",
      "||dW|| (output): 655.9871335214142\n",
      "||dW|| (output): 655.9871335214142\n",
      "||dW|| (output): 655.9871335214145\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214146\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214151\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214146\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214146\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214156\n",
      "||dW|| (output): 655.9871335214156\n",
      "||dW|| (output): 655.9871335214156\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214149\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.987133521416\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214159\n",
      "||dW|| (output): 655.9871335214156\n",
      "||dW|| (output): 655.9871335214157\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.9871335214153\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214152\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.9871335214154\n",
      "||dW|| (output): 655.9871335214144\n",
      "||dW|| (output): 655.9871335214144\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.9871335214141\n",
      "||dW|| (output): 655.9871335214148\n",
      "||dW|| (output): 655.9871335214145\n",
      "||dW|| (output): 655.9871335214142\n",
      "||dW|| (output): 655.987133521415\n",
      "||dW|| (output): 655.9871335214144\n",
      "||dW|| (output): 655.9871335214144\n",
      "||dW|| (output): 655.987133521414\n",
      "||dW|| (output): 655.9871335214148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10dafad50>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode target\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split (optional, keep 80% train for now)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Network setup\n",
    "input_layer = FullyConnectedLayer({'input_dim': 4, 'output_dim': 8, 'activation': 'relu'})\n",
    "hidden_layer = FullyConnectedLayer({'input_dim': 8, 'output_dim': 5, 'activation': 'relu'})\n",
    "output_layer = FullyConnectedLayer({'input_dim': 5, 'output_dim': 3, 'activation': 'softmax'})\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.00005\n",
    "num_epochs = 100\n",
    "loss_curve = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        x = X_train[i]  \n",
    "        y_true = y_train[i]\n",
    "\n",
    "        # Forward pass\n",
    "        input_layer.forward(x)\n",
    "        hidden_layer.forward(input_layer)\n",
    "        output_layer.forward(hidden_layer)\n",
    "\n",
    "        y_pred = output_layer.a\n",
    "        loss = _softmax_crossentropy(y_true, y_pred)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        #print(y_true)\n",
    "        #print(np.round(y_pred, decimals=3))\n",
    "        #print()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_grad = grad(_softmax_crossentropy)\n",
    "        dA = loss_grad(y_true, y_pred)\n",
    "        #print(\"dA:\", dA)\n",
    "\n",
    "        dA_2 = output_layer.backward(dA)\n",
    "        dA_1 = hidden_layer.backward(dA_2)\n",
    "        dA_0 = input_layer.backward(dA_1)\n",
    "\n",
    "        # Update weights\n",
    "        output_layer.step(learning_rate)\n",
    "        hidden_layer.step(learning_rate)\n",
    "        input_layer.step(learning_rate)\n",
    "\n",
    "    print(f\"||dW|| (output): {np.linalg.norm(output_layer.dW)}\")\n",
    "    avg_loss = epoch_loss / X_train.shape[1]\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    loss_curve.append(avg_loss)\n",
    "\n",
    "plt.plot(range(num_epochs),loss_curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
